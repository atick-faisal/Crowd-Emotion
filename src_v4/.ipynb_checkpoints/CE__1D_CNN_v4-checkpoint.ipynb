{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/atick-faisal/Crowd-Emotion/blob/main/src_v4/CE__1D_CNN_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXm9QHXCkyPe",
    "outputId": "f6413725-c390-487f-b656-3146f78ce4d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 25.4MB 1.8MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "l8OfY47r7HXQ",
    "outputId": "9f22d8de-1506-4907-ea97-87a9071578d2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bGoDOAEZ7PuE"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'timestamp'             : str(datetime.datetime.now()),\n",
    "    'model'                 : 'Transfer Learning on YAMNET',\n",
    "    'test_fold'             : 'Fold 5',\n",
    "    'Fs'                    : 44100,\n",
    "    'frame_length'          : 44100,\n",
    "    'frame_step'            : 22050,\n",
    "    'architecture'          : '',\n",
    "    'batch_size'            : 32,\n",
    "    'epochs'                : 0,\n",
    "    'learning_rate'         : 0.000005,\n",
    "    'monitor'               : 'val_loss',\n",
    "    'patience'              : 3,\n",
    "    'class_weight'          : { 0: 0.71, 1:  2.32, 2: 0.86 },\n",
    "    'training_time'         : 0,\n",
    "    'testing_time'          : 0,\n",
    "    'cm_atick'              : '',\n",
    "    'cr_atick'              : '',\n",
    "    'cm_valentina'          : '',\n",
    "    'cr_valentina'          : ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q7sAhoG87SBg"
   },
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR            = os.getcwd()\n",
    "LOG_FILE            = '/content/drive/MyDrive/Research/Crowd Emotion Logs/tl_yamnet.txt'\n",
    "FOLDS               = ['Fold 1', 'Fold 2', 'Fold 3', 'Fold 4', 'Fold 5']\n",
    "EMOTIONS            = ['Approval', 'Disapproval', 'Neutral']\n",
    "\n",
    "ATICK_DATA_DIR      = '/content/Dataset-Atick/'\n",
    "VALENTINA_DATA_DIR  = '/content/Dataset-Valentina/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4F05a8b7XON",
    "outputId": "1a5bf5bf-f006-4119-fa06-233664b0ffaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1HOe5sJe_Juf5uib4f-8pPv-Z64lmqQ4X\n",
      "To: /content/Crowd-Emotion-Dataset.tar.xz\n",
      "239MB [00:01, 151MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=11tC2Nmie9v3ljo60oQJ3sN1rVkorV-N1\n",
      "To: /content/Valentina_CE_Dataset.tar.xz\n",
      "352MB [00:04, 76.0MB/s]\n",
      "CPU times: user 525 ms, sys: 119 ms, total: 644 ms\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# # ----------------- Loading my dataset -------------------\n",
    "# !mkdir /content/Dataset-Atick/\n",
    "# !gdown --id '1HOe5sJe_Juf5uib4f-8pPv-Z64lmqQ4X'\n",
    "# !tar -xf /content/Crowd-Emotion-Dataset.tar.xz -C /content/Dataset-Atick/\n",
    "\n",
    "# # ----------------- Loading Valentina's dataset -------------------\n",
    "# !mkdir /content/Dataset-Valentina/\n",
    "# !gdown --id '11tC2Nmie9v3ljo60oQJ3sN1rVkorV-N1'\n",
    "# !tar -xf /content/Valentina_CE_Dataset.tar.xz -C /content/Dataset-Valentina/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "syVM81JZ7aoW"
   },
   "outputs": [],
   "source": [
    "# --------------- File loading utilities ------------------\n",
    "\n",
    "def load_wav_mono(filename):\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(\n",
    "          file_contents,\n",
    "          desired_channels=1)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    # sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    # wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=CONFIG['Fs'])\n",
    "    return wav\n",
    "\n",
    "def load_wav_for_map(filename, label, fold):\n",
    "  return load_wav_mono(filename), label, fold\n",
    "\n",
    "def load_ds(path, class_names, fold_names=['']):\n",
    "\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    folds = []\n",
    "\n",
    "    for class_name in class_names:\n",
    "        print('processing files for ' + class_name, end=' ... ')\n",
    "\n",
    "        for fold in fold_names:\n",
    "            files_path = os.path.join(path, class_name, fold)\n",
    "\n",
    "            for filename in os.listdir(files_path):\n",
    "                filenames.append(os.path.join(files_path, filename))\n",
    "                labels.append(class_names.index(class_name))\n",
    "\n",
    "                try:\n",
    "                    folds.append(FOLDS.index(fold))\n",
    "                except ValueError:\n",
    "                    folds.append(0)\n",
    "\n",
    "        print('√')\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((filenames, labels, folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZduP_NI7yDc",
    "outputId": "f31551c7-278b-423d-e648-a2eab24c2fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files for Approval ... √\n",
      "processing files for Disapproval ... √\n",
      "processing files for Neutral ... √\n",
      "processing files for Approval ... √\n",
      "processing files for Disapproval ... √\n",
      "processing files for Neutral ... √\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_ds_atick = load_ds(ATICK_DATA_DIR, EMOTIONS, FOLDS)\n",
    "files_ds_valentina = load_ds(VALENTINA_DATA_DIR, EMOTIONS)\n",
    "\n",
    "files_ds_atick.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5nG3XAP70dQ",
    "outputId": "c93c54b2-0e1e-4929-e531-9f118afa38e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_ds_atick = files_ds_atick.map(load_wav_for_map)\n",
    "wav_ds_valentina = files_ds_valentina.map(load_wav_for_map)\n",
    "\n",
    "wav_ds_atick.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "PfopqDwc9BGE"
   },
   "outputs": [],
   "source": [
    "def get_fft_coeffs(wav):\n",
    "    wav = tf.cast(wav, tf.float32)\n",
    "    fft = tf.signal.rfft(wav)\n",
    "    return tf.abs(fft)\n",
    "\n",
    "def frames_map(wav, label, fold):\n",
    "    frames = tf.signal.frame(wav, CONFIG['frame_length'], CONFIG['frame_step'], axis=0)\n",
    "    frames = tf.expand_dims(frames, -1)\n",
    "    num_frames = tf.shape(frames)[0]\n",
    "    return (\n",
    "        frames,\n",
    "        tf.repeat(label, num_frames),\n",
    "        tf.repeat(fold, num_frames)\n",
    "    )\n",
    "\n",
    "def fft_map(frame, label, fold):\n",
    "    return get_fft_coeffs(frame), label, fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uL0GZQy9WhF7",
    "outputId": "d52f2c88-6f93-47da-9466-52ad71104dd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(44100, 1), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_ds_atick = wav_ds_atick.map(frames_map).unbatch()\n",
    "frames_ds_valentina = wav_ds_valentina.map(frames_map).unbatch()\n",
    "\n",
    "frames_ds_atick.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwQHXcWE-ad-",
    "outputId": "2d112fa1-59cf-4ffe-d5b0-09f25249c0a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(44100, 1), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fft_ds_atick = frames_ds_atick.map(fft_map)\n",
    "fft_ds_valentina = frames_ds_valentina.map(fft_map)\n",
    "\n",
    "fft_ds_atick.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "nFiAqukKDEtm"
   },
   "outputs": [],
   "source": [
    "cached_ds_atick = fft_ds_atick.cache()\n",
    "cached_ds_valentina = fft_ds_valentina.cache()\n",
    "\n",
    "train_ds = cached_ds_atick.filter(\n",
    "    lambda frame, label, fold: fold != FOLDS.index(CONFIG['test_fold'])\n",
    ")\n",
    "val_ds = cached_ds_atick.filter(\n",
    "    lambda frame, label, fold: fold == FOLDS.index(CONFIG['test_fold'])\n",
    ")\n",
    "\n",
    "test_ds = cached_ds_valentina\n",
    "\n",
    "# train_ds = cached_ds_atick\n",
    "# val_ds = cached_ds_valentina\n",
    "# test_ds = cached_ds_valentina\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda frame, label, fold: (frame, label)\n",
    "\n",
    "train_ds = train_ds.map(remove_fold_column)\n",
    "val_ds = val_ds.map(remove_fold_column)\n",
    "test_ds = test_ds.map(remove_fold_column)\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000) .batch(CONFIG['batch_size']) \\\n",
    "                    .prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(CONFIG['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().batch(CONFIG['batch_size']).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4SSYZbcDh5o",
    "outputId": "827cc3e2-e419-498c-dc06-7c0d5d21e7fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44100, 1)\n"
     ]
    }
   ],
   "source": [
    "for fft, _, _ in fft_ds_atick.take(1):\n",
    "    input_shape = fft.shape\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "TMNqPlsdDQWi"
   },
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "norm_layer.adapt(fft_ds_atick.map(lambda x, label, fold: x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8hcdxsZYUCK",
    "outputId": "2713bee4-313a-4f89-9b00-36347ee3d680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_1 (Normalizati (None, 44100, 1)          3         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 44098, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 44096, 64)         12352     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 44096, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 22048, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1411072)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                90308672  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 90,321,478\n",
      "Trainable params: 90,321,475\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=input_shape),\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Conv1D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.Conv1D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(EMOTIONS))\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "0spOA7QYbDGP"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=CONFIG['monitor'],\n",
    "    patience=CONFIG['patience'],\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whffQed4cCCk",
    "outputId": "9d34e58f-49c9-486a-ae58-869bcc2a8550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "187/187 [==============================] - 1102s 6s/step - loss: 0.4319 - accuracy: 0.8597 - val_loss: 1.0521 - val_accuracy: 0.5713\n",
      "Epoch 2/300\n",
      "187/187 [==============================] - 1095s 6s/step - loss: 0.6959 - accuracy: 0.6172 - val_loss: 1.0194 - val_accuracy: 0.5765\n",
      "Epoch 3/300\n",
      "133/187 [====================>.........] - ETA: 4:58 - loss: 0.6754 - accuracy: 0.6269"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs              = 300,\n",
    "    validation_data     = val_ds,\n",
    "    callbacks           = callback,\n",
    "    class_weight        = CONFIG['class_weight']\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3yzpUo4cHnl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMxLdJ3Fw/R+9w90gM4KrP/",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "https://github.com/atick-faisal/Crowd-Emotion/blob/main/CE__1D_CNN_v4.ipynb",
   "name": "CE__1D_CNN_v4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
